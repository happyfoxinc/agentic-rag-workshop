{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop Outline\n",
        "\n",
        "\n",
        "1. Introduction\n",
        "   - RAG overview\n",
        "   - LlamaIndex basics\n",
        "\n",
        "2. Building a Simple RAG Chatbot\n",
        "   - Data preparation\n",
        "   - Building a simple Q&A bot\n",
        "   - Simple Q&A bot with conversation history\n",
        "\n",
        "3. Advanced Strategies in RAG\n",
        "    - Strategies while indexing data\n",
        "    - Strategies while querying\n",
        "    - Strategies while answering queries\n",
        "\n",
        "4. Advanced RAG\n",
        "    - Agents with reasoning capabilities\n",
        "\n",
        "5. Building apps with ease with LlamaIndex\n",
        "    - Chat with your PDF\n",
        "    - Building a simple copilot for your website\n",
        "\n",
        "6. Q&A\n"
      ],
      "metadata": {
        "id": "0yeeDlTp2ANr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b4Y5pX6N43Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n"
      ],
      "metadata": {
        "id": "FDRA9JDq3cIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to RAG\n",
        "\n"
      ],
      "metadata": {
        "id": "HYR7gtSj46As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is RAG ?"
      ],
      "metadata": {
        "id": "rrIxpet78AfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data.\n",
        "\n",
        "This is done by retrieving data/documents relevant to a question or task and providing them as context for the LLM.\n",
        "\n",
        "By providing this extra context, LLM can generate answers, that are up-to-date, factually correct, and relevant to a specific domain."
      ],
      "metadata": {
        "id": "iAP9l-Gd_kgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample RAG Architecture"
      ],
      "metadata": {
        "id": "-JkD04Yj_pOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fm-jumpstar.png](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pmaAMJR8_sCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why do we need RAG ?"
      ],
      "metadata": {
        "id": "kCPOWn1__v15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***LLMs do not know your data*** - LLMs are often limited to their pre-trained knowledge and data. Once trained, many LLMs do not have the ability to access data beyond their training data cutoff point.\n",
        "\n",
        "2. ***Factual grounding and consistency*** - LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases."
      ],
      "metadata": {
        "id": "wpF9dv5f_ydB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the use cases of RAG ?"
      ],
      "metadata": {
        "id": "6hR8alMD_7aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Question and answer chatbots***: Incorporating LLMs with chatbots allows them to automatically derive more accurate answers from company documents and knowledge bases. Chatbots are used to automate customer support and website lead follow-up to answer questions and resolve issues quickly.\n",
        "\n",
        "2. ***Search augmentation***: Incorporating LLMs with search engines that augment search results with LLM-generated answers can better answer informational queries and make it easier for users to find the information they need to do their jobs.\n",
        "\n",
        "3. ***Knowledge engine*** — ask questions on your data (e.g., HR, compliance documents): Company data can be used as context for LLMs and allow employees to get answers to their questions easily, including HR questions related to benefits and policies and security and compliance questions."
      ],
      "metadata": {
        "id": "YfN6CAAtAALL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the benefits of RAG ?"
      ],
      "metadata": {
        "id": "2q5PyCGtAcRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG approach has a number of key benefits, including:\n",
        "\n",
        "1. ***Providing up-to-date and accurate responses***: RAG ensures that the response of an LLM is not based solely on static, stale training data. Rather, the model uses up-to-date external data sources to provide responses.\n",
        "2. ***Reducing inaccurate responses, or hallucinations:*** By grounding the LLM model's output on relevant, external knowledge, RAG attempts to mitigate the risk of responding with incorrect or fabricated information (also known as hallucinations). Outputs can include citations of original sources, allowing human verification.\n",
        "3. ***Providing domain-specific, relevant responses***: Using RAG, the LLM will be able to provide contextually relevant responses tailored to an organization's proprietary or domain-specific data.\n",
        "4. ***Being efficient and cost-effective***: Compared to other approaches to customizing LLMs with domain-specific data, RAG is simple and cost-effective. Organizations can deploy RAG without needing to customize the model. This is especially beneficial when models need to be updated frequently with new data.\n"
      ],
      "metadata": {
        "id": "SG1hPdhtAfbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key phases in RAG"
      ],
      "metadata": {
        "id": "7TwghIY0A1WX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![redbricks.jpg](https://www.databricks.com/sites/default/files/inline-images/glossary-rag-image-2.png?v=1704903053)"
      ],
      "metadata": {
        "id": "pFgVjjm-A4Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaIndex basics\n"
      ],
      "metadata": {
        "id": "5c9dgAmeC1f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index\n",
        "%pip install -q llama-index-llms-gemini\n",
        "%pip install -q google-generativeai\n",
        "%pip install -q llama-index-embeddings-gemini\n",
        "%pip install -q rich\n",
        "%pip install -q requests\n",
        "%pip install -q beautifulsoup4"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QVooty1pKLA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concepts in Loading stage"
      ],
      "metadata": {
        "id": "SdT3pyDaHxQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Nodes and Documents](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/):\n",
        "\n",
        "A **Document** is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database.\n",
        "\n",
        "A **Node** is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "y-H8zt6XJek1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import Document, VectorStoreIndex\n",
        "# from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# text_list = [\n",
        "#     \"LlamaIndex provides tools for beginners, advanced users, and everyone in between.\",\n",
        "#     \"Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\"\n",
        "#   ]\n",
        "# documents = [Document(text=t) for t in text_list]\n",
        "\n",
        "# parser = SentenceSplitter()\n",
        "# nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# print(\"Nodes\", nodes)\n",
        "# print(\"Documents\", documents)\n"
      ],
      "metadata": {
        "id": "tNfpNre_J0DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concepts in Indexing stage"
      ],
      "metadata": {
        "id": "MRu6uGP4LZ97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Indexes](https://docs.llamaindex.ai/en/stable/module_guides/indexing/)\n",
        "\n",
        "Once you've ingested your data, LlamaIndex will help you index the data into a structure that's easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\n",
        "\n",
        "\n",
        "\n",
        "> Interested in learning more about indexes ? Visit this site > https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/\n",
        "\n",
        "\n",
        "\n",
        "[Embeddings](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)\n",
        "\n",
        "LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query."
      ],
      "metadata": {
        "id": "TIzzKXMwLpCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://dkharazi.github.io/ecc71bb7c9e227b292dd909b02dbf4e8/embedding.svg)"
      ],
      "metadata": {
        "id": "Zxr3njpV_SJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from google.colab import userdata\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "# embedding = gemini_embedding_model.get_text_embedding(\"Hello, world!\")\n",
        "\n",
        "# print(embedding)"
      ],
      "metadata": {
        "id": "YPFCgSPDLxm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concepts in querying stage"
      ],
      "metadata": {
        "id": "4uekx7eSN3je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Retrievers](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/)\n",
        "\n",
        "A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it's done.\n",
        "\n",
        "[Routers](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)\n",
        "\n",
        "A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate's metadata and the query.\n",
        "\n",
        "[Node Postprocessors](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/)\n",
        "\n",
        "A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
        "\n",
        "[Response Synthesizers](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/)\n",
        "\n",
        "A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
        "\n"
      ],
      "metadata": {
        "id": "LJeoHuxvN6iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.data_structs import Node\n",
        "# from llama_index.core.schema import NodeWithScore\n",
        "# from llama_index.core.response_synthesizers import ResponseMode\n",
        "# from llama_index.core import get_response_synthesizer\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from google.colab import userdata\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# response_synthesizer = get_response_synthesizer(\n",
        "#     response_mode=ResponseMode.COMPACT, llm=gemini_llm\n",
        "# )\n",
        "\n",
        "# response = response_synthesizer.synthesize(\n",
        "#     \"Who was the indian cricket team coach in 2022 ?\", nodes=[NodeWithScore(node=Node(text=\"Rahul dravid was the indian cricket team coach between 2021 and 2024\"), score=1.0)]\n",
        "# )\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "ClavHt9SOWws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a simple RAG Chatbot"
      ],
      "metadata": {
        "id": "04hp9OhRTJj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "EHhGPFj_kBbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scrape some indian cricket players bio-graphy from Wikipedia, which we'll use while building a RAG app.\n",
        "\n",
        "We'll build a chatbot which will answer queries about indian cricket players based on the biography we're scraping off of Wikipedia"
      ],
      "metadata": {
        "id": "Qqsyq2LZGjN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# INDIAN_PLAYERS = [\n",
        "#     \"https://en.wikipedia.org/wiki/MS_Dhoni\",\n",
        "#     \"https://en.wikipedia.org/wiki/Ruturaj_Gaikwad\",\n",
        "#     \"https://en.wikipedia.org/wiki/Shubman_Gill\",\n",
        "#     \"https://en.wikipedia.org/wiki/Prithvi_Shaw\",\n",
        "#     \"https://en.wikipedia.org/wiki/Suryakumar_Yadav\",\n",
        "#     \"https://en.wikipedia.org/wiki/Devdutt_Padikkal\",\n",
        "#     \"https://en.wikipedia.org/wiki/Chetan_Sakariya\",\n",
        "#     \"https://en.wikipedia.org/wiki/Shreyas_Iyer\",\n",
        "#     \"https://en.wikipedia.org/wiki/Yashasvi_Jaiswal\",\n",
        "#     \"https://en.wikipedia.org/wiki/Virat_Kohli\",\n",
        "#     \"https://en.wikipedia.org/wiki/Rohit_Sharma\",\n",
        "#     \"https://en.wikipedia.org/wiki/Shikhar_Dhawan\",\n",
        "#     \"https://en.wikipedia.org/wiki/Lokesh_Rahul\",\n",
        "# ]\n",
        "\n",
        "\n",
        "# def scrape_wiki(url):\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#     name = soup.find(\"h1\", class_=\"firstHeading\").text\n",
        "\n",
        "#     para_tags = soup.find_all(\"p\")\n",
        "#     full_page_text = \"\"\n",
        "\n",
        "#     for para in para_tags:\n",
        "#         full_page_text += para.text\n",
        "\n",
        "#     return name, full_page_text\n",
        "\n",
        "\n",
        "# # Store player bio taken from wikipedia in text files\n",
        "# os.mkdir(\"/content/player_wiki\")\n",
        "\n",
        "# player_data = []\n",
        "# for player in INDIAN_PLAYERS:\n",
        "#     name, text = scrape_wiki(player)\n",
        "#     with open(f\"/content/player_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)\n",
        "\n",
        "\n",
        "# # Look around what information is scraped\n",
        "# files = os.listdir(\"/content/player_wiki\")\n",
        "# for player_name in files[:10]:\n",
        "#   file_path = f\"/content/player_wiki/{player_name}\"\n",
        "\n",
        "#   file_content = None\n",
        "\n",
        "#   with open(file_path) as f:\n",
        "#     file_content = f.read()\n",
        "\n",
        "#   print(player_name.strip(\".txt\"))\n",
        "#   print(file_content[:500], end=\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "ohbYEoUjkFps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scrape IPL Teams information from Wikipedia, which we'll use with player bio for building RAG app."
      ],
      "metadata": {
        "id": "gSnz2VCaGiD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IPL_TEAMS = [\n",
        "#     \"https://en.wikipedia.org/wiki/Chennai_Super_Kings\",\n",
        "#     \"https://en.wikipedia.org/wiki/Delhi_Capitals\",\n",
        "#     \"https://en.wikipedia.org/wiki/Kolkata_Knight_Riders\",\n",
        "#     \"https://en.wikipedia.org/wiki/Mumbai_Indians\",\n",
        "#     \"https://en.wikipedia.org/wiki/Punjab_Kings\",\n",
        "#     \"https://en.wikipedia.org/wiki/Rajasthan_Royals\",\n",
        "#     \"https://en.wikipedia.org/wiki/Royal_Challengers_Bangalore\",\n",
        "#     \"https://en.wikipedia.org/wiki/Sunrisers_Hyderabad\",\n",
        "# ]\n",
        "\n",
        "# # Store team bio taken from wikipedia in text files\n",
        "# os.mkdir(\"/content/team_wiki\")\n",
        "\n",
        "# for team in IPL_TEAMS:\n",
        "#     name, text = scrape_wiki(team)\n",
        "#     with open(f\"./team_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)"
      ],
      "metadata": {
        "id": "TI4X9b_DGhZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a simple Q&A bot"
      ],
      "metadata": {
        "id": "huFJCsy4ld24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now look at how to build a Q&A bot, over the data we've shown above.\n",
        "\n",
        "To do this, we'll use LlamaIndex and Gemini API for generating responses and embeddings for the user queries."
      ],
      "metadata": {
        "id": "VhnvjJfyliOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the documents\n",
        "# index = VectorStoreIndex.from_documents(documents=documents)\n",
        "\n",
        "# query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# while True:\n",
        "#   query = input(\"Enter your query: \")\n",
        "#   if query == \"q\":\n",
        "#     break\n",
        "\n",
        "#   response = query_engine.query(query)\n",
        "#   print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "wCq5CAP_lv0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of this Q&A bot\n"
      ],
      "metadata": {
        "id": "m1OCgASxnnKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Can't handle multiple / complex queries in effectively. Try asking it queries like `With which team did Rohith Sharma win IPL titles with, and what is his highest ODI score is ?`\n",
        "\n",
        "*   Doesn't remember the previous queries and answers, and hence can't handle follow-up questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "EP4GhKjLoVSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Q&A bot with conversation history\n"
      ],
      "metadata": {
        "id": "sRUDxVHvLYlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've highlighted before, the simple Q&A bot we've built before lacks the ability to remember the previous conversations it has had.\n",
        "\n",
        "We'll now look at how to add conversational history to the Q&A bot, and make it a full-fledged chatbot that you can have natural conversations with.\n",
        "\n",
        "🚀 Let's get started !"
      ],
      "metadata": {
        "id": "xPHGzQ8ULdU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "# messages = []\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# # reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# # documents = reader.load_data()\n",
        "\n",
        "# # # Create a in-memory vector store index from the documents\n",
        "# # index = VectorStoreIndex.from_documents(documents=documents)\n",
        "# chat_engine = index.as_chat_engine()\n",
        "\n",
        "# while True:\n",
        "#   query = input(\"Human: \")\n",
        "#   if query == \"q\":\n",
        "#     break\n",
        "\n",
        "#   response = chat_engine.chat(query, chat_history=messages)\n",
        "\n",
        "#   # Store the messages in a in-memory object. This can be persisted to a DB as well.\n",
        "#   messages.append(ChatMessage(role=MessageRole.USER, content=query))\n",
        "#   messages.append(ChatMessage(role=MessageRole.ASSISTANT, content=response))\n",
        "\n",
        "#   print(f\"AI: {response}\")"
      ],
      "metadata": {
        "id": "lhPJTTawNTx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the above we now have a chatbot, that can answer questions in natural language based off on the data we've collected.\n",
        "\n",
        "Now let's proceed with augmenting this chatbot with capabilities to tackle advanced use-cases."
      ],
      "metadata": {
        "id": "2pyd4WeHpzt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U67a0uYIr5ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced strategies in RAG"
      ],
      "metadata": {
        "id": "GYvCeqMIrEoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategies while indexing data"
      ],
      "metadata": {
        "id": "aJvAzoMCrLJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata extraction"
      ],
      "metadata": {
        "id": "KS7T1Lg0s_F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.\n",
        "\n",
        "One method of addressing this is manually labelling each chunk in our dataset or knowledge base. To combat this we can use LLMs to extract some metainformation to help differentiate chunks from each other.\n",
        "\n",
        "We can have LLM's\n",
        "\n",
        "*   Extract specific metadata we're interested in (based on the domain of the documents)\n",
        "*   Extract Q&A pairs from chunks, which we can use while querying.\n",
        "\n"
      ],
      "metadata": {
        "id": "UU0_esoOqwiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see Q&A metadata extraction in action"
      ],
      "metadata": {
        "id": "5kibxYMkrhKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.node_parser import TokenTextSplitter\n",
        "# from llama_index.core.extractors import (\n",
        "#     SummaryExtractor,\n",
        "#     QuestionsAnsweredExtractor,\n",
        "# )\n",
        "# from llama_index.core.schema import MetadataMode\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from llama_index.core.ingestion import IngestionPipeline\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# import json\n",
        "# import nest_asyncio\n",
        "\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Load the data from the folder\n",
        "# reader = SimpleDirectoryReader(input_files=[\"/content/player_wiki/Chetan Sakariya.txt\"])\n",
        "# documents = reader.load_data()\n",
        "\n",
        "# # Split the data into chunks based on empty space\n",
        "# node_parser = TokenTextSplitter(\n",
        "#     separator=\" \", chunk_size=256, chunk_overlap=128\n",
        "# )\n",
        "# original_nodes = node_parser.get_nodes_from_documents(documents[:1])\n",
        "\n",
        "# # Define a Q&A extractor to extract question answer pairs from the documents\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# Settings.llm = gemini_llm\n",
        "# Settings.embed_model = gemini_llm\n",
        "\n",
        "\n",
        "# question_nodes = QuestionsAnsweredExtractor(questions=3, llm=gemini_llm, num_workers=1).extract(nodes=original_nodes)\n",
        "\n",
        "# for i in range(len(original_nodes)):\n",
        "#   print(f\"Node #{i} Content: {original_nodes[i].get_content()}\", end=\"\\n\")\n",
        "#   print(f\"Extracted Questions: {question_nodes[i]}\", end=\"\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "4HSAICWQG248"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic chunking"
      ],
      "metadata": {
        "id": "7MTx0yKDtBEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic chunking is splitting the texts based on embedding similarity i.e., sentences that are closer together in embedding space (or) sentences that have similar meaning are kept together."
      ],
      "metadata": {
        "id": "PCEtZbKHspQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:1400/1*mjbjx2U4d-H21jbCSJZcUA.png)"
      ],
      "metadata": {
        "id": "32VAxOrutzIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does it work ?**\n",
        "\n",
        "- Starts by splitting the text by sentences, and computes embedding for them.\n",
        "- Compares the first sentence's embedding with the second, and if they're within a given threshold, they're kept together.\n",
        "- If the embedding score is not within the threshold, they're separated.\n",
        "- This is repeated, until the entire text is traversed and processed."
      ],
      "metadata": {
        "id": "GrfEYHQps6v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how simple chunking fares with semantic chunking."
      ],
      "metadata": {
        "id": "3Adspj88tXJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import SimpleDirectoryReader\n",
        "# from llama_index.core.node_parser import (\n",
        "#     SentenceSplitter,\n",
        "#     SemanticSplitterNodeParser,\n",
        "# )\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/text-embedding-004\")\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "\n",
        "# # Load document\n",
        "# documents = SimpleDirectoryReader(input_files=[\"/content/player_wiki/Devdutt Padikkal.txt\"]).load_data()\n",
        "\n",
        "# # Setup Semantic Splitter\n",
        "# semantic_splitter = SemanticSplitterNodeParser(\n",
        "#     buffer_size=1, breakpoint_percentile_threshold=95, embed_model=gemini_embedding_model\n",
        "# )\n",
        "\n",
        "# # Setup baseline splitter\n",
        "# base_splitter = SentenceSplitter(chunk_size=128, chunk_overlap=64)\n",
        "\n",
        "# semantic_nodes = semantic_splitter.get_nodes_from_documents(documents, show_progress=True)\n",
        "\n",
        "# base_nodes = base_splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# # Compare both\n",
        "# print(len(semantic_nodes))\n",
        "# print(len(base_nodes))\n",
        "\n",
        "# print(\"Semantic Nodes\")\n",
        "# index_count = 0\n",
        "# for semantic_node in semantic_nodes:\n",
        "#   index_count += 1\n",
        "#   print(f\"Node #{index_count}\")\n",
        "#   print(semantic_node.get_content(), end=\"\\n\")\n",
        "\n",
        "# print(\"Base nodes\")\n",
        "# index_count = 0\n",
        "# for base_node in base_nodes:\n",
        "#   index_count += 1\n",
        "#   print(f\"Node #{index_count}\")\n",
        "#   print(base_node.get_content(), end=\"\\n\")"
      ],
      "metadata": {
        "id": "S8veYIJ1T4rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategies while querying"
      ],
      "metadata": {
        "id": "1xB6dW_SrSzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Routing"
      ],
      "metadata": {
        "id": "SqY9G1IFtKie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Routers are modules that take in a user query and a set of \"choices\" (defined by metadata), and returns one or more selected choices.\n",
        "\n",
        "They are simple but powerful modules that use LLMs for decision making capabilities. They can be used for the following use cases and more:\n",
        "\n",
        "- Selecting the right data source among a diverse range of data sources\n",
        "- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)"
      ],
      "metadata": {
        "id": "_JAu2Kl-utKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:1400/0*Ado23RkTTpNFRO0b)"
      ],
      "metadata": {
        "id": "4gVGIUwvuti_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core.tools import QueryEngineTool\n",
        "# from llama_index.core.query_engine import RouterQueryEngine\n",
        "# from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\", embed_batch_size=50)\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents[:1])\n",
        "# player_query_engine = player_index.as_query_engine()\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents[:1])\n",
        "# team_query_engine = team_index.as_query_engine()\n",
        "\n",
        "# player_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=player_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players.\"\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# team_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=team_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about IPL Teams.\"\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# query_engine = RouterQueryEngine(\n",
        "#     selector=LLMSingleSelector.from_defaults(),\n",
        "#     query_engine_tools=[\n",
        "#         player_tool,\n",
        "#         team_tool,\n",
        "#     ],\n",
        "# )\n",
        "\n",
        "# response = query_engine.query(\"Who is Chetan sakariya ?\")\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "BtIAvcmZW-J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sub-question query engine"
      ],
      "metadata": {
        "id": "LKpqSMK0tR9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-question query engine in simple terms takes in a very complex query , and breaks it down into smaller sub-queries, searches for answers for each of these subqueries, and then generates a final response based on the intermediate answers."
      ],
      "metadata": {
        "id": "oFy1UgN1vDyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://docs.llamaindex.ai/en/v0.10.19/_images/multi_step_diagram.png)"
      ],
      "metadata": {
        "id": "_Ey8ElKAvX9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core.tools import QueryEngineTool\n",
        "# from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents[:1])\n",
        "# player_query_engine = player_index.as_query_engine()\n",
        "\n",
        "# query_engine_tools = [\n",
        "#     QueryEngineTool.from_defaults(\n",
        "#         query_engine=player_query_engine,\n",
        "#         description=\"Useful for answering queries about cricket players\"\n",
        "#     ),\n",
        "# ]\n",
        "\n",
        "# query_engine = SubQuestionQueryEngine.from_defaults(\n",
        "#     query_engine_tools=query_engine_tools,\n",
        "#     use_async=False\n",
        "# )\n",
        "\n",
        "# response = query_engine.query(\n",
        "#     \"How was chetan sakariya's life before and after his IPL debut ?\"\n",
        "# )\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "2l_cE7dQaxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategies while answering queries"
      ],
      "metadata": {
        "id": "4TXj3AGrrW7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up guard rails"
      ],
      "metadata": {
        "id": "aYK3pQNqteH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a bot that can answer queries, it is important we set up guard-rails (or) rules that specify how the bot should interact with the user.\n",
        "\n",
        "Till now, we haven't done it. Go to any of the previous code examples and try asking ***what is distance between the moon and earth***, and it'll answer.\n",
        "\n",
        "When we build and deploy such a bot to customer facing scenarios, we don't really want it to answer these queries. We want it to answer only queries that we're interested in.\n",
        "\n",
        "Let's see how we can do this."
      ],
      "metadata": {
        "id": "l670drE3v_Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core import PromptTemplate\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents[:1])\n",
        "# player_query_engine = player_index.as_query_engine()\n",
        "\n",
        "# response = player_query_engine.query(\"what is 1+1 ?\")\n",
        "\n",
        "# print(\"LLM Response: \", response)\n",
        "\n",
        "# # Implement guard-rail in prompt to avoid the LLM answering a different query\n",
        "# qa_prompt_tmpl_str = (\n",
        "#     \"Context information is below.\\n\"\n",
        "#     \"---------------------\\n\"\n",
        "#     \"{context_str}\\n\"\n",
        "#     \"---------------------\\n\"\n",
        "#     \"Given the context information and not prior knowledge, \"\n",
        "#     \"answer the query.\\n\"\n",
        "#     \"You must only answer queries that are regarding cricket.If the user asks about something else, say I'm sorry, I can't answer that.\\n\"\n",
        "#     \"Query: {query_str}\\n\"\n",
        "#     \"Answer: \"\n",
        "# )\n",
        "# qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "# player_query_engine.update_prompts(\n",
        "#     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        "# )\n",
        "\n",
        "# response = player_query_engine.query(\"what is 1+1 ?\")\n",
        "# print(\"LLM Response after setting up guardrails: \", response)"
      ],
      "metadata": {
        "id": "gI8ESsU-o-Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Still not convinced why we need to setup guardrails , see this https://www.reddit.com/r/OpenAI/comments/18kjwcj/why_pay_indeed/\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7tm_mDFwyWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmenting context with few-shot examples"
      ],
      "metadata": {
        "id": "WYJaU6fItYN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core import PromptTemplate\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents[:1])\n",
        "# player_query_engine = player_index.as_query_engine()\n",
        "\n",
        "# def few_shot_examples(**kwargs):\n",
        "#   return  \"\"\"\n",
        "#     Question: Which team did chetan Sakariya play for ?\n",
        "#     Answer: He played for Saurashtra in Ranji trophy.\n",
        "\n",
        "#     Question: Which IPL Teams did Sakariya play for ?\n",
        "#     Answer: He played for rajasthan royals and Delhi Capitals.\n",
        "#   \"\"\"\n",
        "\n",
        "# qa_prompt_tmpl_str = \"\"\"\\\n",
        "# Context information is below.\n",
        "# ---------------------\n",
        "# {context_str}\n",
        "# ---------------------\n",
        "# Given the context information and not prior knowledge, \\\n",
        "# answer the query asking about citations over different topics.\n",
        "# Please provide your answer in the form of a structured JSON format containing \\\n",
        "# a list of authors as the citations. Some examples are given below.\n",
        "\n",
        "# {few_shot_examples}\n",
        "\n",
        "# Query: {query_str}\n",
        "# Answer: \\\n",
        "# \"\"\"\n",
        "\n",
        "# qa_prompt_tmpl = PromptTemplate(\n",
        "#     qa_prompt_tmpl_str,\n",
        "#     function_mappings={\"few_shot_examples\": few_shot_examples},\n",
        "# )\n",
        "\n",
        "# player_query_engine.update_prompts(\n",
        "#     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        "# )\n",
        "\n",
        "\n",
        "# response = player_query_engine.query(\"Teams that Sakariya played for ?\")\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "99KL1UDercgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MCyzJ5qEr6zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG"
      ],
      "metadata": {
        "id": "IXviLE3KrcG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents with reasoning capabilities - ReAct framework"
      ],
      "metadata": {
        "id": "Cs_MMV6Xtnvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ReAct*** is technique which enable LLMs to do reasoning and take task specific actions. It combines chain of thought reasoning with action planning.It enables LLMs to generate reasoning traces and task-specific actions, leveraging the synergy between them. This approach demonstrates superior performance over baselines in various tasks, overcoming issues like hallucination and error propagation."
      ],
      "metadata": {
        "id": "D-pLQbQuxiNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://statusneo.com/wp-content/uploads/2024/01/fe9fa1ac-dfde-4d91-8b5b-4497b742c414_1400x686.jpg)"
      ],
      "metadata": {
        "id": "qTW5EM0axnRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.agent import ReActAgent\n",
        "# from llama_index.core.tools import  QueryEngineTool\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.0-pro\", api_key=gemini_api_key)\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_files=[\"/content/player_wiki/Ruturaj Gaikwad.txt\"])\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents)\n",
        "# player_query_engine = player_index.as_query_engine()\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_files=[\"/content/team_wiki/Chennai Super Kings.txt\"])\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents)\n",
        "# team_query_engine = team_index.as_query_engine()\n",
        "\n",
        "# player_query_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=player_query_engine,\n",
        "#     name=\"player_query_tool\",\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players\"\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# team_query_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=team_query_engine,\n",
        "#     name=\"team_query_tool\",\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about IPL Teams\"\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# agent = ReActAgent.from_tools([player_query_tool, team_query_tool], verbose=True)\n",
        "\n",
        "# response = agent.chat(\"Can you tell me who's the new captain of CSK, and share a brief history of his playing career\")\n",
        "# # response = agent.chat(\"Compare and contrast the performance of CSK before and after the 2021 season.\")\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "Oj1liYpd4ndJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building apps with ease with LlamaIndex"
      ],
      "metadata": {
        "id": "CweX62g4tt_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with your PDF"
      ],
      "metadata": {
        "id": "jiU14HojroEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "nZcBTJJzyGUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pymupdf"
      ],
      "metadata": {
        "id": "VHRyuEJj9Vmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading a sample PDF"
      ],
      "metadata": {
        "id": "tcTTz4rwyJZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O test.pdf https://arxiv.org/pdf/2406.11652"
      ],
      "metadata": {
        "id": "eZndEk5OHsTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's write some code"
      ],
      "metadata": {
        "id": "HoLiufZ9yQpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# from llama_index.core import download_loader\n",
        "# from llama_index.core.indices import VectorStoreIndex\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from google.colab import userdata\n",
        "# from llama_index.readers.file import PyMuPDFReader\n",
        "\n",
        "# # from llama_index.readers.file.unstructured import UnstructuredReader\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "\n",
        "# loader = PyMuPDFReader()\n",
        "# documents = loader.load(file_path=\"/content/test.pdf\")\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(documents=documents)\n",
        "\n",
        "# query_engine = index.as_query_engine()\n",
        "\n",
        "# response = query_engine.query(\"What Performance indicators were used ?\")\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "YP6-HH9h9T25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a simple copilot for your website"
      ],
      "metadata": {
        "id": "dTi_sPAertse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-readers-web"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IDZ5fkdGD7xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import VectorStoreIndex\n",
        "# from llama_index.readers.web import SimpleWebPageReader\n",
        "# from google.colab import userdata\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash-latest\", api_key=gemini_api_key)\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "\n",
        "# documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "#     [\"https://www.thecricketmonthly.com/story/1442710/baracara-s-speed-demon--the-shamar-joseph-story\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(documents)\n",
        "# query_engine = index.as_query_engine()\n",
        "\n",
        "# response = query_engine.query(\"Who is shamar joseph?\")\n",
        "\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "iQyiBGqmDvAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q & A's"
      ],
      "metadata": {
        "id": "YdIZ4XJFty5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![test.png](https://thumb.ac-illust.com/86/862a3ccb34edb9da6cd42cffbeeb0bfa_t.jpeg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vR8d4XSbJRpn"
      }
    }
  ]
}