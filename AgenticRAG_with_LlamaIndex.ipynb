{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Large language models"
      ],
      "metadata": {
        "id": "8QWoW68ho9wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are large language models ?"
      ],
      "metadata": {
        "id": "lSvEBZOo1qEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models are advanced artificial intelligence programs that can understand and generate human-like text. They're trained on vast amounts of written material from the internet and other sources, allowing them to process and produce language on a wide range of topics."
      ],
      "metadata": {
        "id": "_-1k1qwYpAiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While they can produce remarkably human-like responses, they don't truly understand or think like humans do. Instead, they excel at recognizing and replicating language patterns to generate appropriate responses to prompts or questions."
      ],
      "metadata": {
        "id": "5BdH2hpnpEiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://www.investopedia.com/thmb/ulGrKT5WnVclGMOgQQVe65OtmeI=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/large-language-model-7563532-final-9e350e9fa02d4685887aa061af7a2de2.png)"
      ],
      "metadata": {
        "id": "x0XTEEuOpHZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do LLM's work ?"
      ],
      "metadata": {
        "id": "KyfgPxKk1wkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models are trained on massive datasets of text using a process called machine learning. They analyze patterns in this data to understand how language is structured and used. During training, the model is repeatedly shown examples of text and learns to predict what words or phrases are likely to come next. This process involves adjusting millions or even billions of internal parameters to improve its predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "_f5UdC0j1ylw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As training progresses, the model becomes better at understanding context, grammar, and even subtle nuances in language. Once trained, when given a prompt or question, the model uses its learned patterns to generate relevant and coherent responses. It's like the model has developed a complex map of language, allowing it to navigate and produce human-like text on a vast array of topics."
      ],
      "metadata": {
        "id": "MXi6QRab2FYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:2000/1*faLf-OAINgRAyMyCLyZLvg.png)"
      ],
      "metadata": {
        "id": "HaXpwXna2GIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capabilities and Uses of LLM's"
      ],
      "metadata": {
        "id": "cSGt1AKy2X-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models (LLMs) have a wide range of applications across various fields, thanks to their ability to understand and generate human-like text. These AI-powered tools can assist with numerous language-related tasks, enhancing productivity and creativity in many areas. Here are some key capabilities and use-cases of LLMs:\n",
        "\n",
        "- Text generation (stories, articles, code)\n",
        "- Question answering\n",
        "- Language translation\n",
        "- Summarization\n",
        "- Sentiment analysis\n",
        "- Code completion and generation\n",
        "\n",
        "These applications demonstrate the versatility of LLMs in handling diverse language-based tasks, from creative writing to technical analysis. As the technology continues to evolve, we can expect to see even more innovative uses emerge in various industries and everyday life."
      ],
      "metadata": {
        "id": "ZHWw2RlF3Lo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://www.baeldung.com/wp-content/uploads/sites/4/2023/05/Foundation-Models.jpg)"
      ],
      "metadata": {
        "id": "PG3-JsTg3WmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the LLMs that are available today ?"
      ],
      "metadata": {
        "id": "QNvzpKQO36i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several prominent large language models (LLMs) are available today, each with unique features and capabilities."
      ],
      "metadata": {
        "id": "SR6Ho43p4DJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **OpenAI GPT series**: Includes GPT-3 and GPT-4, known for versatility and wide-ranging applications.\n",
        "- **Google Bard (LaMDA)**: Renowned for its conversational abilities.\n",
        "- **Meta's LLaMA**: Focuses on efficient performance.\n",
        "- **Claude by Anthropic**: Emphasizes safety and ethical use.\n",
        "- **Mistral series**: Prioritizes open-source development.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hIpx6Oxh4dbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:2000/1*8kymcqfvPfQzfhVF6fUaHA.png)"
      ],
      "metadata": {
        "id": "GYwsuNsW4nY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where can I try them out ?"
      ],
      "metadata": {
        "id": "Sb2eNbYx5Mdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try chatting with some of these LLM's in the following websites"
      ],
      "metadata": {
        "id": "T2e67Zbu5RSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://claude.ai/\n",
        "- https://chatgpt.com/\n",
        "- https://www.meta.ai/"
      ],
      "metadata": {
        "id": "zYitVKez5WwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Gemini"
      ],
      "metadata": {
        "id": "qCJhTviP5yZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM2.\n",
        "\n"
      ],
      "metadata": {
        "id": "I93dQbn250XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gemini.jpg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Google_Gemini_logo.svg/440px-Google_Gemini_logo.svg.png)"
      ],
      "metadata": {
        "id": "HVzd1zlY56_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models available in Gemini family"
      ],
      "metadata": {
        "id": "2b586Ze06RA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the models available in Gemini 1 family\n",
        "\n",
        "- Gemini Ultra (Most intelligent, costly, and Slow)\n",
        "- Gemini Pro (Middle ground in intelligent, cost, and speed)\n",
        "- Gemini Flash (Least intelligent, cheapest and fastest)\n",
        "- Gemini Nano (Used for ondevice with low memory footprint)\n",
        "\n",
        "These are the models available in Gemini 1.5 family\n",
        "\n",
        "- Gemini flash 1.5\n",
        "- Gemini Pro 1.5"
      ],
      "metadata": {
        "id": "GibmwdGn57zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Try out Gemini at > ***https://gemini.google.com/***\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp2d736t6IxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Gemini"
      ],
      "metadata": {
        "id": "XyP32k1i6AE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google offers a free-tier API to play around / work with Gemini. We'll be using Gemini to build our chatbot."
      ],
      "metadata": {
        "id": "am0zXXY66vQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get an API token to get started"
      ],
      "metadata": {
        "id": "O_lF31U564uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting an API token"
      ],
      "metadata": {
        "id": "Q1DUDjt96gbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Visit this site (https://ai.google.dev/gemini-api)\n",
        "2. Login with your personal Google account.\n",
        "3. Click **\"Get API key in Google studio\"**\n",
        "4. Create a new API Key\n",
        "5. Copy it, and go to Secrets in colab, create a new key titled **GOOGLE_API_KEY** and paste it.\n",
        "\n"
      ],
      "metadata": {
        "id": "nTycjz7FygVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the required dependencies"
      ],
      "metadata": {
        "id": "ENuEw_tm7cYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to install the required python libraries for us to work with Gemini"
      ],
      "metadata": {
        "id": "hc-XEOP97hB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index\n",
        "%pip install -q llama-index-llms-gemini\n",
        "%pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "9FoIXukG5IvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple completion"
      ],
      "metadata": {
        "id": "5tZAntqkRqiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Large Language Models (LLMs)** can generate text based on a given prompt or input. This is called a completion. Think of it like autofill, but for sentences or even entire conversations! For example, if you type \"**The capital of France is**\", an LLM can complete it with \"**Paris**\".\n",
        "\n"
      ],
      "metadata": {
        "id": "fZRAPSkH8Sv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you start telling a story with \"**Once upon a time in a land far, far away**\", an LLM can continue the story with its own ideas.\n",
        "\n",
        "Completions can be used for tasks like writing assistance, or even generating code - the possibilities are endless!"
      ],
      "metadata": {
        "id": "tY_Dvk3G8ZTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://storage.googleapis.com/download.tensorflow.org/tflite/examples/autocomplete_fig2.gif)"
      ],
      "metadata": {
        "id": "o6L1a6Q59GzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now try out some completions with Gemini. Uncomment the below cells, and try out some completions"
      ],
      "metadata": {
        "id": "RlI5kT7V8fh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Read the API Key from Google colab secrets, and assign it to a variable\n",
        "# gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# # Initialise the Gemini client with the API Key we got in the previous step\n",
        "# response = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-flash\").complete(\"What national team does Virat Kohli play for ?\")\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "QT0d8aoXRTz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Completion"
      ],
      "metadata": {
        "id": "dnQiSzXhRshA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models (LLMs) offer advanced chat completion capabilities, enabling highly interactive and dynamic conversations. These models, such as OpenAI's GPT-4, Google's Bard (LaMDA), and Meta's LLaMA, can understand and generate human-like responses, making them ideal for customer support, virtual assistants, and educational tools."
      ],
      "metadata": {
        "id": "6KBcjjbw92sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try chatting with Gemini via an API"
      ],
      "metadata": {
        "id": "27_rBLUL94cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.llms import ChatMessage\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Get the chat history\n",
        "# messages = [\n",
        "#     ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "#     ChatMessage(role=\"assistant\", content=\"Hi there friend. What can I help you with today ?\"),\n",
        "#     ChatMessage(\n",
        "#         role=\"user\", content=\"What is hello in spanish ?\"\n",
        "#     ),\n",
        "# ]\n",
        "\n",
        "# # Initialise the Gemini client with API key, and pass along the chat history\n",
        "# response = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-flash\").chat(messages)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "ZqmIANXORZ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "Ef4vQoQMRviV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming with large language models (LLMs) means they can respond to you right away, just like a live conversation. This is important for things like chatbots, virtual assistants, and translation services, where quick answers are needed. Streaming makes sure you get fast and smooth responses, making the interaction feel more natural."
      ],
      "metadata": {
        "id": "45HzbyVFOcz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://cdn.shopify.com/s/files/1/0779/4361/files/SidekickStreaming-Gif-02-v003.gif?v=1690295248)"
      ],
      "metadata": {
        "id": "P1iEHDMhOnDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now replicate the above experience with Gemini"
      ],
      "metadata": {
        "id": "8lHQvfooOpae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Initialise the Gemini client with API key\n",
        "# llm = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-pro\")\n",
        "\n",
        "# # Call the streaming API endpoint\n",
        "# resp = llm.stream_complete(\n",
        "#     \"Write about Virat Kohli in about 1000 words\"\n",
        "# )\n",
        "# for r in resp:\n",
        "#     print(r.text, end=\"\")\n",
        "#     sys.stdout.flush()\n",
        "#     time.sleep(0.2)\n"
      ],
      "metadata": {
        "id": "MFunA2AuReoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a simple bot\n",
        "\n"
      ],
      "metadata": {
        "id": "tpjRTFUEPu61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on what we've learned before, let's put together a simple bot with streaming support"
      ],
      "metadata": {
        "id": "QMIM5KJTP0kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Initialise the Gemini client\n",
        "# llm = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "# while True:\n",
        "#     # Collect the query from the user\n",
        "#     query = input(\"Enter your query: \")\n",
        "#     if query == \"q\":\n",
        "#       break\n",
        "\n",
        "#     # Make a streaming API call to get answer to user query\n",
        "#     resp = llm.stream_complete(query)\n",
        "#     for r in resp:\n",
        "#         print(r.text, end=\"\")\n",
        "#         sys.stdout.flush()\n",
        "#         time.sleep(0.2)"
      ],
      "metadata": {
        "id": "Vf6W1ynKQHgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking queries about real life events"
      ],
      "metadata": {
        "id": "8zK7zEFdUsHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you try asking the bot questions around \"**Who won the T20 world cup in 2024?**\" it will struggle to answer it."
      ],
      "metadata": {
        "id": "fP4S3YCvUvD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   It'll either say it hasn't happened yet\n",
        "2.   (or) It'll make up an answer, which is not factual\n",
        "\n"
      ],
      "metadata": {
        "id": "owk8v-Z6U4Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because\n",
        "\n",
        "1. ***LLMs do not know your data*** - LLMs are often limited to their pre-trained knowledge and data. Once trained, many LLMs do not have the ability to access data beyond their training data cutoff point.\n",
        "\n",
        "2. ***Factual grounding and consistency*** - LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases."
      ],
      "metadata": {
        "id": "wpF9dv5f_ydB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see how to overcome this limitation of LLM's"
      ],
      "metadata": {
        "id": "_nj829QVVUy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Initialising the Gemini Client\n",
        "# llm = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "# PROMPT = \"\"\"\n",
        "# Here is some information which could help in answering the user question.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Using the above information, you can answer the user's question.\n",
        "# Here is the user's question:\n",
        "\n",
        "# {user_question}\n",
        "# \"\"\"\n",
        "\n",
        "# def get_context():\n",
        "#     return \"India won the T20 World cup in 2024.\"\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     query = input(\"Enter your query: \")\n",
        "#     if query == \"q\":\n",
        "#       break\n",
        "\n",
        "#     # Inject the information into the prompt, we send to the LLM\n",
        "#     prompt = PROMPT.format(context=get_context(), user_question=query)\n",
        "\n",
        "#     resp = llm.stream_complete(prompt=prompt)\n",
        "#     for r in resp:\n",
        "#         print(r.text, end=\"\")\n",
        "#         sys.stdout.flush()\n",
        "#         time.sleep(0.2)"
      ],
      "metadata": {
        "id": "IoKYLxowU22a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By feeding the LLM with relevant information that is needed to answer the user's question, LLM can now provide answers to the queries, which it wasn't able to answer previously, since it wasn't trained on that data."
      ],
      "metadata": {
        "id": "tjUFsyvOWCFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now ask our bot question around \"**Who were all the players who were part of the indian cricket team that won the T20 world cup?**\""
      ],
      "metadata": {
        "id": "H9SAQSlfW8D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously the bot isn't able to answer it. It lacks this information. Let's now add this information as well."
      ],
      "metadata": {
        "id": "oe0kfRkNXxCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Initialise the Gemini client\n",
        "# llm = Gemini(api_key=gemini_api_key, model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "# PROMPT = \"\"\"\n",
        "# Here is some information which could help in answering the user question.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Using the above information, you can answer the user's question.\n",
        "# Here is the user's question:\n",
        "\n",
        "# {user_question}\n",
        "# \"\"\"\n",
        "\n",
        "# def get_context():\n",
        "#     # Add information about who won the world cup, and the squad details as well.\n",
        "#     return \"\"\"\n",
        "#     India won the 2024 T20 World cup.\n",
        "\n",
        "#     These were the indian team players who were part of the squad that won the 2024 T20 World Cup:\n",
        "#     1. Rohit Sharma\n",
        "#     2. Virat Kohli\n",
        "#     3. Rishabh Pant\n",
        "#     4. Sanju Samson\n",
        "#     5. Yashasvi Jaiswal\n",
        "#     6. Surayakumar Yadav\n",
        "#     7. Shivam Dube\n",
        "#     8. Hardik Pandya\n",
        "#     9. Axar Patel\n",
        "#     10. Ravindra Jadeja\n",
        "#     11. Jasprit Bumrah\n",
        "#     12. Mohammed Siraj\n",
        "#     13. Kuldeep Yadav\n",
        "#     14. Yuzvendra Chahal\n",
        "#     15. Arshdeep Singh\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     # Collect the user query\n",
        "#     query = input(\"Enter your query: \")\n",
        "#     if query == \"q\":\n",
        "#         break\n",
        "\n",
        "#     # Inject necessary information into the prompt, which is fed to the LLM\n",
        "#     prompt = PROMPT.format(context=get_context(), user_question=query)\n",
        "#     resp = llm.stream_complete(prompt=prompt)\n",
        "#     for r in resp:\n",
        "#         print(r.text, end=\"\")\n",
        "#         sys.stdout.flush()\n",
        "#         time.sleep(0.2)"
      ],
      "metadata": {
        "id": "t3GrtukiZ6w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try asking \"**What was the score of the first match that india played in the T20 world cup ?**\" , the bot will not be able to answer."
      ],
      "metadata": {
        "id": "hga1sSKFZ9vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we'd have to add that information to the prompt. LLM's have limited context length. We can't keep on passing all this information to the LLM's everytime."
      ],
      "metadata": {
        "id": "jUyrxVtCaQrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we start increasing the amount of information that we pass along to the LLM,\n",
        "\n",
        "\n",
        "\n",
        "1.   **The response times start increasing**\n",
        "2.   **The cost (which is based on number of characters we send in the prompt) also increases**\n",
        "\n",
        "Apart from this, LLM's do have a limit on number of characters, that we could pass along. It is finite !\n",
        "\n",
        "So we have to be frugal about what information we're sending to the LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "PRStNmKNavhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a given query, retrieving the relevant information, that needs to be passed along to the LLM is called as **RAG (Retrieval Augmented generation)**."
      ],
      "metadata": {
        "id": "hzZn38HBde3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to RAG"
      ],
      "metadata": {
        "id": "yRhyXjljXls6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is RAG ?"
      ],
      "metadata": {
        "id": "ogaGDBgUdyqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data.\n",
        "\n",
        "This is done by retrieving data/documents relevant to a question or task and providing them as context for the LLM.\n",
        "\n",
        "By providing this extra context, LLM can generate answers, that are up-to-date, factually correct, and relevant to a specific domain."
      ],
      "metadata": {
        "id": "3qu-CrO8dZya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fm-jumpstar.png](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)\n"
      ],
      "metadata": {
        "id": "FwCA-B0ndwBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Phases in RAG"
      ],
      "metadata": {
        "id": "sjzONYqgd0_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key phases in RAG are:\n",
        "\n",
        "\n",
        "\n",
        "1. Data preparation\n",
        "2. Data indexing\n",
        "3. Information retrieval\n",
        "4. LLM Inference (Answer generation)\n",
        "\n"
      ],
      "metadata": {
        "id": "lkXY0jrQd90d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![redbricks.jpg](https://www.databricks.com/sites/default/files/inline-images/glossary-rag-image-2.png?v=1704903053)"
      ],
      "metadata": {
        "id": "tXs-iM-Pd35y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the benefits of RAG ?"
      ],
      "metadata": {
        "id": "kBFiS669ePMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG approach has a number of key benefits, including:\n",
        "\n",
        "1. ***Providing up-to-date and accurate responses***: RAG ensures that the response of an LLM is not based solely on static, stale training data. Rather, the model uses up-to-date external data sources to provide responses.\n",
        "2. ***Reducing inaccurate responses, or hallucinations:*** By grounding the LLM model's output on relevant, external knowledge, RAG attempts to mitigate the risk of responding with incorrect or fabricated information (also known as hallucinations). Outputs can include citations of original sources, allowing human verification.\n",
        "3. ***Providing domain-specific, relevant responses***: Using RAG, the LLM will be able to provide contextually relevant responses tailored to an organization's proprietary or domain-specific data.\n",
        "4. ***Being efficient and cost-effective***: Compared to other approaches to customizing LLMs with domain-specific data, RAG is simple and cost-effective. Organizations can deploy RAG without needing to customize the model. This is especially beneficial when models need to be updated frequently with new data.\n"
      ],
      "metadata": {
        "id": "qZ5Ki9O9eR8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where can we use RAG ?"
      ],
      "metadata": {
        "id": "L1Vlqt0teUfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Question and answer chatbots***: Incorporating LLMs with chatbots allows them to automatically derive more accurate answers from company documents and knowledge bases. Chatbots are used to automate customer support and website lead follow-up to answer questions and resolve issues quickly.\n",
        "\n",
        "2. ***Search augmentation***: Incorporating LLMs with search engines that augment search results with LLM-generated answers can better answer informational queries and make it easier for users to find the information they need to do their jobs."
      ],
      "metadata": {
        "id": "8-uh61OheWve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a chatbot with RAG which can handle player related queries"
      ],
      "metadata": {
        "id": "jxHGh3_refKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the required dependencies"
      ],
      "metadata": {
        "id": "R8x5rjc5eie-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index\n",
        "%pip install -q llama-index-llms-gemini\n",
        "%pip install -q google-generativeai\n",
        "%pip install -q llama-index-embeddings-gemini\n",
        "%pip install -q requests\n",
        "%pip install -q beautifulsoup4"
      ],
      "metadata": {
        "id": "VHbVlnm_uSnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the necessary data"
      ],
      "metadata": {
        "id": "3ixQgXowuTJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scrape some indian cricket players bio-graphy from Wikipedia, which we'll use while building a RAG app.\n",
        "\n",
        "We'll build a chatbot which will answer queries about indian cricket players based on the biography we're scraping off of Wikipedia"
      ],
      "metadata": {
        "id": "9RXmrG7kuiSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# INDIAN_PLAYERS = [\n",
        "#     \"https://en.wikipedia.org/wiki/Suryakumar_Yadav\",\n",
        "#     \"https://en.wikipedia.org/wiki/Yashasvi_Jaiswal\",\n",
        "#     \"https://en.wikipedia.org/wiki/Virat_Kohli\",\n",
        "#     \"https://en.wikipedia.org/wiki/Rohit_Sharma\",\n",
        "#     \"https://en.wikipedia.org/wiki/Hardik_Pandya\",\n",
        "#     \"https://en.wikipedia.org/wiki/Ravindra_Jadeja\",\n",
        "#     \"https://en.wikipedia.org/wiki/Axar_Patel\",\n",
        "#     \"https://en.wikipedia.org/wiki/Kuldeep_Yadav\",\n",
        "#     \"https://en.wikipedia.org/wiki/Jasprit_Bumrah\",\n",
        "# ]\n",
        "\n",
        "\n",
        "# def scrape_wiki(url):\n",
        "#     # Make an API call to get the page content\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#     name = soup.find(\"h1\", class_=\"firstHeading\").text\n",
        "\n",
        "#     # Extract all the text from para HTML tags\n",
        "#     para_tags = soup.find_all(\"p\")\n",
        "#     full_page_text = \"\"\n",
        "#     for para in para_tags:\n",
        "#         full_page_text += para.text\n",
        "\n",
        "#     return name, full_page_text\n",
        "\n",
        "\n",
        "# # Store player bio taken from wikipedia in text files\n",
        "# if not os.path.exists(\"/content/player_wiki\"):\n",
        "#     os.mkdir(\"/content/player_wiki\")\n",
        "\n",
        "# player_data = []\n",
        "# for player in INDIAN_PLAYERS:\n",
        "#     name, text = scrape_wiki(player)\n",
        "#     with open(f\"/content/player_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)\n",
        "\n",
        "\n",
        "# # Look around what information is scraped\n",
        "# files = os.listdir(\"/content/player_wiki\")\n",
        "# for player_name in files[:10]:\n",
        "#   file_path = f\"/content/player_wiki/{player_name}\"\n",
        "\n",
        "#   file_content = None\n",
        "\n",
        "#   with open(file_path) as f:\n",
        "#     file_content = f.read()\n",
        "\n",
        "#   print(player_name.strip(\".txt\"))\n",
        "#   print(file_content[:100], end=\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "MTbx1UHLue2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a chatbot"
      ],
      "metadata": {
        "id": "q5ilGudS-Jvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.core import SimpleDirectoryReader\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# documents = reader.load_data()\n",
        "\n",
        "# PROMPT = \"\"\"\n",
        "# Here is some information which could help in answering the user question.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Using the above information, you can answer the user's question.\n",
        "# Here is the user's question:\n",
        "\n",
        "# {user_question}\n",
        "# \"\"\"\n",
        "\n",
        "# def get_context(query_text):\n",
        "#     # Add a simple keyword search over the documents.\n",
        "#     # Documents which have lot of keywords matching the query_text will be returned.\n",
        "#     # This is a simple way to retrieve the context information.\n",
        "#     context = \"\"\n",
        "#     words = query_text.split()\n",
        "\n",
        "#     # Remove stop words\n",
        "#     stop_words = [\"is\", \"the\", \"who\", \"what\", \"where\", \"when\", \"why\", \"how\"]\n",
        "#     words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "#     doc_score_map = {}\n",
        "#     for word in words:\n",
        "#         for doc in documents:\n",
        "#             if word in doc.text:\n",
        "#                 doc_score_map[doc.doc_id] = doc_score_map.get(doc.doc_id, 0) + 1\n",
        "\n",
        "#     if doc_score_map:\n",
        "#         doc_id = max(doc_score_map, key=doc_score_map.get)\n",
        "\n",
        "#         for doc in documents:\n",
        "#             if doc.doc_id == doc_id:\n",
        "#                 context = doc.text\n",
        "#                 break\n",
        "\n",
        "#     print(\"\\033[34mRetrieved context: \", context, end=\"\\n\\n\\033[0m\")\n",
        "#     return context\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     query = input(\"Enter your query: \")\n",
        "#     if query == \"q\":\n",
        "#         break\n",
        "\n",
        "#     prompt = PROMPT.format(context=get_context(query), user_question=query)\n",
        "#     response = gemini_llm.stream_complete(prompt=prompt)\n",
        "\n",
        "#     print(\"Answer: \", end=\"\")\n",
        "#     for r in response:\n",
        "#         print(r.text, end=\"\")\n",
        "#         sys.stdout.flush()\n",
        "#         time.sleep(0.2)"
      ],
      "metadata": {
        "id": "nT4ufUVj90x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, we built a chatbot which used a simple keyword search which worked okay in some cases, and which didn't work good in some cases.\n",
        "\n",
        "Keyword search is not the best way to search for information among a large set of information."
      ],
      "metadata": {
        "id": "lGkpXDD5x40o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keyword search, while useful, has several limitations that can affect its effectiveness. Here are some common pitfalls of keyword search:\n",
        "\n",
        "- **Lack of context:** Keyword search often misses the broader meaning or intent behind a query, focusing only on specific words.\n",
        "- **Synonym confusion:** It may miss relevant results that use synonyms or related terms instead of the exact keywords.\n",
        "- **Ambiguity:** Words with multiple meanings can lead to irrelevant results.\n",
        "Overemphasis on popularity: Popular content may overshadow more relevant but less well-known information.\n",
        "- **Missing conceptual matches:** Keyword search struggles with finding content that's conceptually related but doesn't contain the exact search terms.\n",
        "- **Over-reliance on exact matches:** This can lead to missing valuable information that's phrased differently."
      ],
      "metadata": {
        "id": "X5iIHksJyIy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of simply searching of relevant information based on the keywords alone, what if we're able to understand the information that is stored, and we're able to retrieve the most relevant information based on the query ?"
      ],
      "metadata": {
        "id": "sWQ10yVoyNv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where **semantic search** comes in."
      ],
      "metadata": {
        "id": "nUvHk_PRygnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our chatbot truly understand meaning of the queries"
      ],
      "metadata": {
        "id": "RaPmqMSKuaMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic search"
      ],
      "metadata": {
        "id": "xsEzcu9Ezo43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic search is an advanced approach to finding information that goes beyond simple keyword matching. Instead of just looking for exact words, it tries to understand the meaning and context of your search query. This method is like having a smart librarian who doesn't just match book titles, but understands what you're really looking for.:"
      ],
      "metadata": {
        "id": "_Ch397Pkyw1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic search can pick up on synonyms, related concepts, and even the intent behind your question. This makes it much better at finding relevant information, even when the exact words in your search don't appear in the results."
      ],
      "metadata": {
        "id": "MVWyIPHuzS5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector embeddings"
      ],
      "metadata": {
        "id": "DzRSwGqVzr68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector embeddings are a key technology that makes semantic search possible. They're a way of turning words, sentences, or even entire documents into long lists of numbers. These numbers represent the meaning of the text in a way that computers can understand and compare."
      ],
      "metadata": {
        "id": "ecofbszpztmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine if you could turn the meaning of every word into a unique point in space. Words with similar meanings would be close together, while very different words would be far apart."
      ],
      "metadata": {
        "id": "KobvrQFazwK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector embeddings work in a similar way, allowing computers to measure how similar or different pieces of text are in terms of their meaning, not just their spelling. This helps search engines find results that are truly relevant to what you're looking for, even if they use different words to express the same idea."
      ],
      "metadata": {
        "id": "r7_UscZYzzCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://dkharazi.github.io/ecc71bb7c9e227b292dd909b02dbf4e8/embedding.svg)"
      ],
      "metadata": {
        "id": "6qEl_CCS0Uad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating embeddings with Gemini"
      ],
      "metadata": {
        "id": "H80FuDCi0ZKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand embeddings, let's try to put it to use. Let's use embedding models that are available in Gemini family to generate embeddings"
      ],
      "metadata": {
        "id": "pzLf27eK0d79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "\n",
        "# # Initialise the Gemini embedding model Client with the API Key\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# embedding = gemini_embedding_model.get_text_embedding(\"Hello, world!\")\n",
        "# print(embedding)"
      ],
      "metadata": {
        "id": "VNBXyS940c6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swapping keyword search with Vector search"
      ],
      "metadata": {
        "id": "Xr5Mb7nau3l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a chatbot, that uses vector search instead of keyword search using LlamaIndex"
      ],
      "metadata": {
        "id": "WjmyGM9Xxwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import time\n",
        "\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the documents\n",
        "# index = VectorStoreIndex.from_documents(documents=documents, similarity_top_k=3, show_progress=True)\n",
        "# index_retriever = index.as_retriever()\n",
        "\n",
        "# PROMPT = \"\"\"\n",
        "# Here is some information which could help in answering the user question.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Using the above information, you can answer the user's question.\n",
        "# Here is the user's question:\n",
        "\n",
        "# {user_question}\n",
        "# \"\"\"\n",
        "\n",
        "# def get_context(query_text):\n",
        "#     relevant_docs =  index_retriever.retrieve(query_text)\n",
        "\n",
        "#     context = \"\"\n",
        "#     for doc in relevant_docs:\n",
        "#         context += doc.get_content()\n",
        "\n",
        "#     print(\"\\033[34mRetrieved context: \", context, end=\"\\n\\n\\033[0m\")\n",
        "#     return context\n",
        "\n",
        "\n",
        "# while True:\n",
        "#     query = input(\"Enter your query: \")\n",
        "#     if query == \"q\":\n",
        "#         break\n",
        "\n",
        "#     prompt = PROMPT.format(context=get_context(query), user_question=query)\n",
        "\n",
        "#     print(\"Answer: \", end=\"\")\n",
        "#     resp = gemini_llm.stream_complete(prompt=prompt)\n",
        "#     for r in resp:\n",
        "#         print(r.text, end=\"\")\n",
        "#         sys.stdout.flush()\n",
        "#         time.sleep(0.2)"
      ],
      "metadata": {
        "id": "yxP4IyKW083H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LlamaIndex end-to-end instead of just for querying"
      ],
      "metadata": {
        "id": "M6jT-EUF2vps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using LlamaIndex just for querying, let's use LlamaIndex to build our chatbot."
      ],
      "metadata": {
        "id": "MSqH7wA524jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# # reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# # documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the documents\n",
        "# # index = VectorStoreIndex.from_documents(documents=documents, show_progress=True, verbose=True)\n",
        "\n",
        "# # Construct a query engine on top of the index\n",
        "# query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# while True:\n",
        "#   query = input(\"Enter your query: \")\n",
        "#   if query == \"q\":\n",
        "#     break\n",
        "\n",
        "#   response = query_engine.query(query)\n",
        "#   print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "_Yg2VeU13HCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding ability for our chatbot to answer national team related queries"
      ],
      "metadata": {
        "id": "R8XobzQe4xGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading necessary data"
      ],
      "metadata": {
        "id": "iE8rQC3x4_FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NATIONAL_CRICKET_TEAMS = [\n",
        "#     \"https://en.wikipedia.org/wiki/Australia_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/England_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/India_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/New_Zealand_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/Pakistan_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/South_Africa_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/Sri_Lanka_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/West_Indies_cricket_team\",\n",
        "# ]\n",
        "\n",
        "# # Store team bio taken from wikipedia in text files\n",
        "# if not os.path.exists(\"/content/team_wiki\"):\n",
        "#     os.mkdir(\"/content/team_wiki\")\n",
        "\n",
        "# for team in NATIONAL_CRICKET_TEAMS:\n",
        "#     name, text = scrape_wiki(team)\n",
        "#     with open(f\"./team_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)"
      ],
      "metadata": {
        "id": "XUjofdt25Bvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up our bot to answer queries related to teams"
      ],
      "metadata": {
        "id": "k030CUfo44mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "\n",
        "# # Initialise the gemini embedding model to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the documents\n",
        "# index = VectorStoreIndex.from_documents(documents=documents, show_progress=True, verbose=True)\n",
        "\n",
        "# # Construct a query engine on top of the index\n",
        "# query_engine = index.as_query_engine(similarity_top_k=3, streaming=True)\n",
        "\n",
        "# while True:\n",
        "#   query = input(\"Enter your query: \")\n",
        "#   if query == \"q\":\n",
        "#     break\n",
        "\n",
        "#   response = query_engine.query(query)\n",
        "#   print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "IpSQDSHZ7I07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding routing to our bot to determine which source needs to be queried"
      ],
      "metadata": {
        "id": "F4-B2c_q7OBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our bot can either handle queries about players (or) about the national teams depending on the data source that is being considered."
      ],
      "metadata": {
        "id": "ZR5Y4qwi7ZMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want our bot to answer questions from both of these data sources ? What if our bot could choose the appropriate data source, based on the query ?"
      ],
      "metadata": {
        "id": "GX0t92ls72Zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a simple router, which determines which data source (index) to use based on the query from the user."
      ],
      "metadata": {
        "id": "ccbRjdud8AAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:1400/0*Ado23RkTTpNFRO0b)"
      ],
      "metadata": {
        "id": "DgTOuZhx8Go6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "# from llama_index.core.tools import QueryEngineTool\n",
        "# from llama_index.core.query_engine import RouterQueryEngine\n",
        "# from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\", embed_batch_size=50)\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents, show_progress=True)\n",
        "# player_query_engine = player_index.as_query_engine(streaming=True)\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents, show_progress=True)\n",
        "# team_query_engine = team_index.as_query_engine(streaming=True)\n",
        "\n",
        "# player_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=player_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players. Contains data about players taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"player_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# team_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=team_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about national cricket teams. Contains data about teams taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"team_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# query_engine = RouterQueryEngine(\n",
        "#     selector=LLMSingleSelector.from_defaults(),\n",
        "#     query_engine_tools=[\n",
        "#         player_tool,\n",
        "#         team_tool,\n",
        "#     ],\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# response = query_engine.query(\"Who is Virat Kohli ?\")\n",
        "# response.print_response_stream()\n"
      ],
      "metadata": {
        "id": "nysynSFT8UHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our bot handle multi-turn conversations"
      ],
      "metadata": {
        "id": "Hlko2kUy3hg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-turn conversations are dialogues that extend beyond a single exchange, requiring participants to remember and build upon previous messages. For chatbots, memory is essential to engage in these more complex and natural interactions. Without memory, a chatbot would treat each user message as an isolated query, leading to disjointed and often frustrating exchanges."
      ],
      "metadata": {
        "id": "RUTzkaS63kYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By incorporating memory, chatbots can:\n",
        "\n",
        "- **Follow conversation threads**: Keep track of the current topic and refer back to earlier points.\n",
        "- **Understand context**: Interpret user messages correctly based on the conversation history.\n",
        "- **Provide relevant responses**: Offer information that builds on previous exchanges.\n",
        "- **Handle follow-up questions**: Respond appropriately to queries that depend on earlier context.\n",
        "- **Maintain coherence**: Ensure the conversation flows logically from one turn to the next."
      ],
      "metadata": {
        "id": "5grOZOYn3uQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory enables chatbots to participate in fluid, multi-turn dialogues that more closely resemble human conversation"
      ],
      "metadata": {
        "id": "px4JwVGR4bgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now look at how to add conversational history to our bot, and make it a full-fledged chatbot that you can have natural conversations with."
      ],
      "metadata": {
        "id": "SfG8vl514hcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's get started !"
      ],
      "metadata": {
        "id": "A_RAA_mu8fd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "# from llama_index.core.llms import ChatMessage, MessageRole\n",
        "# from llama_index.core.chat_engine import ContextChatEngine\n",
        "# from llama_index.core.retrievers import RouterRetriever\n",
        "# from llama_index.core.tools import RetrieverTool\n",
        "# from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents)\n",
        "# player_retriever = player_index.as_retriever()\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents)\n",
        "# team_retriever = team_index.as_retriever()\n",
        "\n",
        "# player_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=player_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players. Contains data about players taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"player_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# team_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=team_query_engine,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about national cricket teams. Contains data about teams taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"team_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# retriever = RouterRetriever(\n",
        "#     selector=LLMSingleSelector.from_defaults(),\n",
        "#     retriever_tools=[\n",
        "#         player_tool,\n",
        "#         team_tool,\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Initialise a chat memory buffer component\n",
        "# memory = ChatMemoryBuffer.from_defaults(token_limit=150000)\n",
        "\n",
        "# # Initialise chat engine with the retriever, LLM and the memory component\n",
        "# chat_engine = ContextChatEngine(retriever=retriever, prefix_messages=[], llm=gemini_llm, memory=memory)\n",
        "\n",
        "# while True:\n",
        "#   query = input(\"Human: \")\n",
        "#   if query == \"q\":\n",
        "#     break\n",
        "\n",
        "#   # Pass along the user query to the chat engine\n",
        "#   response = chat_engine.stream_chat(query)\n",
        "#   print(\"AI: \", end=\"\")\n",
        "#   response.print_response_stream()"
      ],
      "metadata": {
        "id": "RDRIb1mS4leu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mzDoNcBDGyZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling complex questions"
      ],
      "metadata": {
        "id": "2r7A4WDH4k7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now, our bot can\n",
        "\n",
        "1. Answer queries about players, and teams\n",
        "2. Determine which data source needs to be used, depending on the query.\n",
        "3. Handle multi-turn conversations."
      ],
      "metadata": {
        "id": "Ntwoh8sg8vTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can't handle very complex queries like \"**Who is the current indian team captain ? Which IPL team does he play for ? What is his birth date ?**\""
      ],
      "metadata": {
        "id": "ilog58hv88UY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this, we'll have to break down the query into multiple sub-questions, which we need to answer in order, and then answer the entire question.\n",
        "\n",
        "In the above example, we can break down the above question into:\n",
        "\n",
        "1. Who is the current indian team captain ?\n",
        "2. Once we figure out that the india team captain is Rohit Sharma, we should then get an answer for \"IPL team that Rohith Sharma plays for\".\n",
        "3. Then figure out his birth date.\n",
        "4. Answer the entire question.\n"
      ],
      "metadata": {
        "id": "iezuw7MF9IwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://docs.llamaindex.ai/en/v0.10.19/_images/multi_step_diagram.png)"
      ],
      "metadata": {
        "id": "FhM0Q4Xi91Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "# from llama_index.core.llms import ChatMessage, MessageRole\n",
        "# from llama_index.core.retrievers import RouterRetriever\n",
        "# from llama_index.core.tools import RetrieverTool\n",
        "# from llama_index.core.agent import ReActAgent\n",
        "\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents)\n",
        "# player_retriever = player_index.as_retriever()\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents)\n",
        "# team_retriever = team_index.as_retriever()\n",
        "\n",
        "# player_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=player_retriever,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players. Contains data about players taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"player_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# team_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=team_retriever,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about national cricket teams. Contains data about teams taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"team_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# # use ReAct Agent\n",
        "# agent = ReActAgent.from_tools(\n",
        "#     tools = [player_tool, team_tool],\n",
        "#     llm=gemini_llm,\n",
        "#     verbose=True\n",
        "# )"
      ],
      "metadata": {
        "id": "n5n94UxW94NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # use ReAct Agent\n",
        "# agent = ReActAgent.from_tools(\n",
        "#     tools = [player_tool, team_tool],\n",
        "#     llm=gemini_llm,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# response = agent.chat(\"Who is the current indian team captain ? Which IPL team does he play for ?\")"
      ],
      "metadata": {
        "id": "MUzj4FRA_OzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our bot now can also handle complex questions \n",
        "\n",
        "But, if you think about, the data we fed to our bot contains only the basic information about them and their teams.\n",
        "\n",
        "But, what if you want the bot answer live scores ? or get scoredcards from past matches ?\n",
        "\n",
        "There are free Cricket APIs that can provide us with this data.\n",
        "\n",
        "But, as you know LLMs cannot perform API calls. Now, how do we solve this ?"
      ],
      "metadata": {
        "id": "1WsogFO4_aDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bridging the Gap with Tools\n",
        "\n",
        "To overcome these limitations and enhance the capabilities of LLMs, researchers and developers have introduced the concept of \"tools.\" These tools are external systems or functions that LLMs can leverage to accomplish specific parts of a given task.\n",
        "\n",
        "Tools in the context of LLMs are:\n",
        "- External APIs, software, or functions\n",
        "- Specialized systems designed for specific tasks\n",
        "- Interfaces that allow LLMs to interact with the real world or access current data"
      ],
      "metadata": {
        "id": "De4rVu_HT9T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool-Using Agents\n",
        "\n",
        "Tool-using agents are AI systems designed to interact with and utilize various tools or external resources to accomplish tasks more effectively. These agents combine the power of language models with the ability to use specific tools, bridging the gap between general language understanding and specialized task execution."
      ],
      "metadata": {
        "id": "BQyX_4tMPoos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Agent with an API calling tool\n",
        "\n",
        "### What are we going to build ?\n",
        "\n",
        "Let's try to build a tool which can call an API which will give us match level info and scorecards for the T20 World cup.\n",
        "\n",
        "I will be using a free cricket data API service called [Cricket Data](https://cricketdata.org)\n",
        "\n",
        "*Steps to get the API Key*\n",
        "\n",
        "1. Sign up for an account here [Signup](https://cricketdata.org/signup.aspx).\n",
        "2. Verify your email and login.\n",
        "3. You should be able to see your Free API key in the dashboard page."
      ],
      "metadata": {
        "id": "uo5PJJRJBi-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the tools to call the API.\n",
        "\n",
        "Let's build our first tool get all the matches list for the T20 world cup."
      ],
      "metadata": {
        "id": "QRn_DMMuEGjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from llama_index.core.tools import FunctionTool\n",
        "\n",
        "# # this is a constant series id for the series we are\n",
        "# # going to fetch the info for.\n",
        "# SERIES_ID = \"e079ef23-b5e9-4802-93e9-dd2f27db0533\"\n",
        "\n",
        "# CRICKET_API = userdata.get(\"CRICKET_API\")\n",
        "\n",
        "# def get_matches_list():\n",
        "#   \"\"\"\n",
        "#   Return the list of Matches and their information\n",
        "#   for the 2024 T20 world cup\n",
        "#   \"\"\"\n",
        "#   url = f\"https://api.cricapi.com/v1/series_info?apikey={CRICKET_API}&id={SERIES_ID}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "# get_matches_list_tool = FunctionTool.from_defaults(fn=get_matches_list)\n"
      ],
      "metadata": {
        "id": "sA_xEC04Ec7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly test our tool by providing it to an Agent."
      ],
      "metadata": {
        "id": "uHyuEI2UJP4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.agent import ReActAgent\n",
        "\n",
        "# GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# llm = Gemini(\n",
        "#     api_key=GOOGLE_API_KEY,\n",
        "#     model=\"models/gemini-1.5-flash\"\n",
        "# )\n",
        "\n",
        "# api_agent = ReActAgent.from_tools([get_matches_list_tool], llm=llm, verbose=True, max_iterations=50)"
      ],
      "metadata": {
        "id": "y7Ubx9DBFAFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ask a simple question\n",
        "# response = api_agent.chat(\"Who played the first game in the series ?\")"
      ],
      "metadata": {
        "id": "_-2HHIEQCmiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, now our bot is capable of giving us match results. But, I also want the match specific info and score cards. Well, let build two more tools that can,\n",
        "\n",
        "1. Get us match specific scorecard.\n",
        "\n",
        "2. Get us all the team squads in the series."
      ],
      "metadata": {
        "id": "8yjjG5EdDDvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from llama_index.core.tools import FunctionTool\n",
        "# from llama_index.core.agent import ReActAgent\n",
        "\n",
        "# # this is a constant series id for the series we are\n",
        "# # going to fetch the info for.\n",
        "# SERIES_ID = \"e079ef23-b5e9-4802-93e9-dd2f27db0533\"\n",
        "\n",
        "# CRICKET_API = userdata.get(\"CRICKET_API\")\n",
        "\n",
        "# def get_matches_list():\n",
        "#   \"\"\"\n",
        "#   Return the list of Matches and their information\n",
        "#   for the 2024 T20 world cup\n",
        "#   \"\"\"\n",
        "#   url = f\"https://api.cricapi.com/v1/series_info?apikey={CRICKET_API}&id={SERIES_ID}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "\n",
        "# def get_match_scorecard(match_id: str):\n",
        "#   \"\"\"\n",
        "#   Returns score card for a specific match for the 2024 T20 world cup\n",
        "#   Args\n",
        "#    - match_id - UUID of the match which can be obtained using get_matches_list\n",
        "#   \"\"\"\n",
        "#   url = url = f\"https://api.cricapi.com/v1/match_scorecard?apikey={CRICKET_API}&id={match_id}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "\n",
        "# def get_series_squad():\n",
        "#   \"\"\"\n",
        "#   Returns team squads for the 2024 T20 world cup\n",
        "#   \"\"\"\n",
        "#   url = url = f\"https://api.cricapi.com/v1/series_squad?apikey={CRICKET_API}&id={SERIES_ID}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "# get_match_scorecard_tool = FunctionTool.from_defaults(fn=get_match_scorecard)\n",
        "# get_matches_list_tool = FunctionTool.from_defaults(fn=get_matches_list)\n",
        "# get_series_squad_tool = FunctionTool.from_defaults(fn=get_series_squad)\n",
        "\n",
        "# GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# llm = Gemini(\n",
        "#     api_key=GOOGLE_API_KEY,\n",
        "#     model=\"models/gemini-1.5-flash\"\n",
        "# )\n",
        "\n",
        "# api_agent = ReActAgent.from_tools([get_matches_list_tool, get_series_squad_tool, get_match_scorecard_tool], llm=llm, verbose=True, max_iterations=50)"
      ],
      "metadata": {
        "id": "2bRZUsLtDmVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's interact with our agent."
      ],
      "metadata": {
        "id": "sMB8Qe_ZEFv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# response = api_agent.chat(\"Who won the T20 world cup ? what's the scorecard ?\")"
      ],
      "metadata": {
        "id": "R_cHhIlFEKC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = api_agent.chat(\"Give me the Indian squad that played in the T20 world cup\")"
      ],
      "metadata": {
        "id": "Y1Bfe_MuE4qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our bot is capable doing so many more things. Kudos to tools.\n",
        "\n",
        "But, there is still one thing missing. If you guys are \"Cricket Nerds\" like me , you might want to post series analysis on how each player performed.\n",
        "\n",
        "You can do this with APIs but calling the match specific API for each match and trying to analyse it very tedious.\n",
        "\n",
        "Let's say we have a CSV dataset of all player stats frkm the T20 world cup.\n",
        "\n",
        "Let's build an analysis agent "
      ],
      "metadata": {
        "id": "2asSs4mAFGHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build a tool that can analyse CSV dataset and perform operations\n",
        "\n",
        "This agent will be Able to answer from a CSV which contains player statistics from T20 world cup."
      ],
      "metadata": {
        "id": "5F9E38HQ5TVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required dependencies\n",
        "\n",
        "- pandas: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language."
      ],
      "metadata": {
        "id": "W-vZGUwHjIT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install requried dependencies\n",
        "%pip install pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "az8XA6LriMYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the required dataset\n",
        "\n",
        "Our dataset is available in Github - https://raw.githubusercontent.com/happyfoxinc/agentic-rag-workshop/main/data/ODI_data.csv"
      ],
      "metadata": {
        "id": "PLnuyceTROJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O batting_stats.csv https://raw.githubusercontent.com/happyfoxinc/agentic-rag-workshop/main/data/batting_stats_for_icc_mens_t20_world_cup_2024.csv\n",
        "!wget -O bowling_stats.csv https://raw.githubusercontent.com/happyfoxinc/agentic-rag-workshop/main/data/bowling_stats_for_icc_mens_t20_world_cup_2024.csv"
      ],
      "metadata": {
        "id": "ORscpBk1RWy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data\n",
        "\n",
        "Let's import the data and take a look at what we are going to work with\n",
        "\n",
        "We are goint to work with 2 different datasets which is related to the recent T20 world cup which India won \n",
        "\n",
        "1. Batting stats\n",
        "2. Bowling stats"
      ],
      "metadata": {
        "id": "ONHMPyim8pdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # load data using pandas\n",
        "# import pandas as pd\n",
        "\n",
        "# batting_df = pd.read_csv(\"/content/batting_stats.csv\")\n",
        "\n",
        "# # see what we are dealing with\n",
        "# batting_df.head()"
      ],
      "metadata": {
        "id": "RCv2h244ovrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bowling_df = pd.read_csv(\"/content/bowling_stats.csv\")\n",
        "\n",
        "# bowling_df.head()"
      ],
      "metadata": {
        "id": "R69QJFkS_Jpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a tool that can interact with our dataset\n",
        "\n",
        "Now, lets build a simple tool that can get some insights from the data that we have. Let's say you want,\n",
        "\n",
        "For the batting stats dataset,\n",
        "1. Get the player highest no of runs\n",
        "\n",
        "2. Player with the highest strike rate\n",
        "\n",
        "For the bowling stats dataset,\n",
        "\n",
        "1. Get the player with the highest number of wickets\n",
        "\n",
        "2. Get the player with the lowest economy"
      ],
      "metadata": {
        "id": "18IypvJLtofo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.tools import FunctionTool\n",
        "# from llama_index.core.agent import ReActAgent\n",
        "\n",
        "# def get_t20_world_cup_stat_insights(stats_type: str, insight_type: str):\n",
        "#   \"\"\"\n",
        "#   This will return the requested insights for the the given stats_type\n",
        "#   and insight_type\n",
        "\n",
        "#   Supported stats_type:\n",
        "#   1. batting\n",
        "#   2. bowling\n",
        "\n",
        "#   Supported insight_types\n",
        "#   1. For batting\n",
        "#     a. highest_run_scorer\n",
        "#     b. player_with_highest_strike_rate\n",
        "#   2. For bowling\n",
        "#     a. highest_wicket_taker\n",
        "#     b. player_with_least_economy\n",
        "#   \"\"\"\n",
        "#   if stats_type == \"batting\":\n",
        "#     if insight_type == \"highest_run_scorer\":\n",
        "#       row_with_highest_score = batting_df.loc[batting_df[\"Runs\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_score.to_dict()\n",
        "#     elif insight_type == \"player_with_highest_strike_rate\":\n",
        "#       row_with_highest_strike_rate = batting_df.loc[batting_df[\"SR\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_strike_rate.to_dict()\n",
        "#     else:\n",
        "#       return \"Invalid insight_type for batting\"\n",
        "#   elif stats_type == \"bowling\":\n",
        "#     if insight_type == \"highest_wicket_taker\":\n",
        "#       row_with_highest_wickets = bowling_df.loc[bowling_df[\"Wkts\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_wickets.to_dict()\n",
        "#     elif insight_type == \"player_with_least_economy\":\n",
        "#       row_with_least_economy = bowling_df.loc[bowling_df[\"Econ\"].idxmin()]\n",
        "\n",
        "#       return row_with_least_economy.to_dict()\n",
        "#     else:\n",
        "#       return \"Invalid insight_type for bowling\"\n",
        "#   else:\n",
        "#     return \"Invalid stats_type\"\n",
        "\n",
        "\n",
        "# get_t20_world_cup_stat_insights_tool = FunctionTool.from_defaults(fn=get_t20_world_cup_stat_insights)\n",
        "\n",
        "# # initialize our agent\n",
        "# GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# llm = Gemini(\n",
        "#     api_key=GOOGLE_API_KEY,\n",
        "#     model=\"models/gemini-1.5-flash\"\n",
        "# )\n",
        "\n",
        "# agent = ReActAgent.from_tools([get_t20_world_cup_stat_insights_tool], llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "8q6xvMiQtgKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's interact with our agent"
      ],
      "metadata": {
        "id": "0Y6vPJN3IQCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# response = agent.chat(\"Who was the best run scorer and how many runs did he score ?\")"
      ],
      "metadata": {
        "id": "8BOfNfeuITtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = agent.chat(\"Who has the best strike rate ?\")"
      ],
      "metadata": {
        "id": "UFaf-IcnIX1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = agent.chat(\"Compare and contrast between the player with highest score and highest strike rate\")"
      ],
      "metadata": {
        "id": "yMiGO_NWIae8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = agent.chat(\"Who took the most wickets and who has the least economy in the tournament ?\")"
      ],
      "metadata": {
        "id": "JBZEgZGVIcah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all these tools in place, we can\n",
        "\n",
        "1. Get match specific info\n",
        "\n",
        "2. Get the squads for each team\n",
        "\n",
        "3. Analyse each player statistics\n",
        "\n",
        "Now, let's go ahead and wire these up with out chatbot "
      ],
      "metadata": {
        "id": "30xAzHojIe4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing our RAG Chatbot with tools\n",
        "\n",
        "By building the above we were able to get statistics for the T20 world via both CSV and an API.\n",
        "\n",
        "Now, let's provide add these capabilities to our existing chatbot by providing it with these tools."
      ],
      "metadata": {
        "id": "upp5AONXy4S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# INDIAN_PLAYERS = [\n",
        "#     \"https://en.wikipedia.org/wiki/Suryakumar_Yadav\",\n",
        "#     \"https://en.wikipedia.org/wiki/Yashasvi_Jaiswal\",\n",
        "#     \"https://en.wikipedia.org/wiki/Virat_Kohli\",\n",
        "#     \"https://en.wikipedia.org/wiki/Rohit_Sharma\",\n",
        "#     \"https://en.wikipedia.org/wiki/Hardik_Pandya\",\n",
        "#     \"https://en.wikipedia.org/wiki/Ravindra_Jadeja\",\n",
        "#     \"https://en.wikipedia.org/wiki/Axar_Patel\",\n",
        "#     \"https://en.wikipedia.org/wiki/Kuldeep_Yadav\",\n",
        "#     \"https://en.wikipedia.org/wiki/Jasprit_Bumrah\",\n",
        "# ]\n",
        "\n",
        "\n",
        "# def scrape_wiki(url):\n",
        "#     # Make an API call to get the page content\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#     name = soup.find(\"h1\", class_=\"firstHeading\").text\n",
        "\n",
        "#     # Extract all the text from para HTML tags\n",
        "#     para_tags = soup.find_all(\"p\")\n",
        "#     full_page_text = \"\"\n",
        "#     for para in para_tags:\n",
        "#         full_page_text += para.text\n",
        "\n",
        "#     return name, full_page_text\n",
        "\n",
        "\n",
        "# # Store player bio taken from wikipedia in text files\n",
        "# if not os.path.exists(\"/content/player_wiki\"):\n",
        "#     os.mkdir(\"/content/player_wiki\")\n",
        "\n",
        "# player_data = []\n",
        "# for player in INDIAN_PLAYERS:\n",
        "#     name, text = scrape_wiki(player)\n",
        "#     with open(f\"/content/player_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)\n",
        "\n",
        "# NATIONAL_CRICKET_TEAMS = [\n",
        "#     \"https://en.wikipedia.org/wiki/Australia_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/England_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/India_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/New_Zealand_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/Pakistan_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/South_Africa_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/Sri_Lanka_national_cricket_team\",\n",
        "#     \"https://en.wikipedia.org/wiki/West_Indies_cricket_team\",\n",
        "# ]\n",
        "\n",
        "# # Store team bio taken from wikipedia in text files\n",
        "# if not os.path.exists(\"/content/team_wiki\"):\n",
        "#     os.mkdir(\"/content/team_wiki\")\n",
        "\n",
        "# for team in NATIONAL_CRICKET_TEAMS:\n",
        "#     name, text = scrape_wiki(team)\n",
        "#     with open(f\"./team_wiki/{name}.txt\", \"x\") as f:\n",
        "#         f.write(text)\n"
      ],
      "metadata": {
        "id": "wBIDSg33cEm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from llama_index.llms.gemini import Gemini\n",
        "# from llama_index.core.agent import ReActAgent\n",
        "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from google.colab import userdata\n",
        "# from llama_index.core import Settings\n",
        "# from llama_index.core.tools import QueryEngineTool\n",
        "# from llama_index.core.tools import FunctionTool\n",
        "\n",
        "# # download dataset\n",
        "\n",
        "\n",
        "# # Initialise the gemini embedding model client to be used for search\n",
        "# gemini_embedding_model = GeminiEmbedding(api_key=gemini_api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "# # Initialise Gemini Flash LLM client to be used for generating answers\n",
        "# gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "# # Mark the LLM and embedding model to be used as default\n",
        "# Settings.embed_model = gemini_embedding_model\n",
        "# Settings.llm = gemini_llm\n",
        "\n",
        "# # Read the player data from the directory\n",
        "# reader = SimpleDirectoryReader(input_dir=\"/content/player_wiki/\")\n",
        "# player_documents = reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# player_index = VectorStoreIndex.from_documents(documents=player_documents)\n",
        "# player_retriever = player_index.as_retriever()\n",
        "\n",
        "# # Read the team data from the directory\n",
        "# team_reader = SimpleDirectoryReader(input_dir=\"/content/team_wiki/\")\n",
        "# team_documents = team_reader.load_data()\n",
        "\n",
        "# # Create a in-memory vector store index from the player documents\n",
        "# team_index = VectorStoreIndex.from_documents(documents=team_documents)\n",
        "# team_retriever = team_index.as_retriever()\n",
        "\n",
        "# player_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=player_retriever,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about specific cricket players. Contains data about players taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"player_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# team_tool = RetrieverTool.from_defaults(\n",
        "#     retriever=team_retriever,\n",
        "#     description=(\n",
        "#         \"Useful for queries that are about national cricket teams. Contains data about teams taken from their wikipedia page.\"\n",
        "#     ),\n",
        "#     name=\"team_wiki_tool\"\n",
        "# )\n",
        "\n",
        "# # this is a constant series id for the series we are\n",
        "# # going to fetch the info for.\n",
        "# SERIES_ID = \"e079ef23-b5e9-4802-93e9-dd2f27db0533\"\n",
        "\n",
        "# CRICKET_API = userdata.get(\"CRICKET_API\")\n",
        "\n",
        "# def get_matches_list():\n",
        "#   \"\"\"\n",
        "#   Return the list of Matches and their information\n",
        "#   for the 2024 T20 world cup\n",
        "#   \"\"\"\n",
        "#   url = f\"https://api.cricapi.com/v1/series_info?apikey={CRICKET_API}&id={SERIES_ID}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "\n",
        "# def get_match_scorecard(match_id: str):\n",
        "#   \"\"\"\n",
        "#   Returns score card for a specific match for the 2024 T20 world cup\n",
        "#   Args\n",
        "#    - match_id - UUID of the match which can be obtained using get_matches_list\n",
        "#   \"\"\"\n",
        "#   url = url = f\"https://api.cricapi.com/v1/match_scorecard?apikey={CRICKET_API}&id={match_id}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "\n",
        "# def get_series_squad():\n",
        "#   \"\"\"\n",
        "#   Returns team squads for the 2024 T20 world cup\n",
        "#   \"\"\"\n",
        "#   url = url = f\"https://api.cricapi.com/v1/series_squad?apikey={CRICKET_API}&id={SERIES_ID}\"\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   return response.json()\n",
        "\n",
        "# def get_t20_world_cup_stat_insights(stats_type: str, insight_type: str):\n",
        "#   \"\"\"\n",
        "#   This will return the requested insights for the the given stats_type\n",
        "#   and insight_type\n",
        "\n",
        "#   Supported stats_type:\n",
        "#   1. batting\n",
        "#   2. bowling\n",
        "\n",
        "#   Supported insight_types\n",
        "#   1. For batting\n",
        "#     a. highest_run_scorer\n",
        "#     b. player_with_highest_strike_rate\n",
        "#   2. For bowling\n",
        "#     a. highest_wicket_taker\n",
        "#     b. player_with_least_economy\n",
        "#   \"\"\"\n",
        "#   if stats_type == \"batting\":\n",
        "#     if insight_type == \"highest_run_scorer\":\n",
        "#       row_with_highest_score = batting_df.loc[batting_df[\"Runs\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_score.to_dict()\n",
        "#     elif insight_type == \"player_with_highest_strike_rate\":\n",
        "#       row_with_highest_strike_rate = batting_df.loc[batting_df[\"SR\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_strike_rate.to_dict()\n",
        "#     else:\n",
        "#       return \"Invalid insight_type for batting\"\n",
        "#   elif stats_type == \"bowling\":\n",
        "#     if insight_type == \"highest_wicket_taker\":\n",
        "#       row_with_highest_wickets = bowling_df.loc[bowling_df[\"Wkts\"].idxmax()]\n",
        "\n",
        "#       return row_with_highest_wickets.to_dict()\n",
        "#     elif insight_type == \"player_with_least_economy\":\n",
        "#       row_with_least_economy = bowling_df.loc[bowling_df[\"Econ\"].idxmin()]\n",
        "\n",
        "#       return row_with_least_economy.to_dict()\n",
        "#     else:\n",
        "#       return \"Invalid insight_type for bowling\"\n",
        "#   else:\n",
        "#     return \"Invalid stats_type\"\n",
        "\n",
        "\n",
        "# get_t20_world_cup_stat_insights_tool = FunctionTool.from_defaults(fn=get_t20_world_cup_stat_insights)\n",
        "# get_match_scorecard_tool = FunctionTool.from_defaults(fn=get_match_scorecard)\n",
        "# get_matches_list_tool = FunctionTool.from_defaults(fn=get_matches_list)\n",
        "# get_series_squad_tool = FunctionTool.from_defaults(fn=get_series_squad)\n",
        "\n",
        "# GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# llm = Gemini(\n",
        "#     api_key=GOOGLE_API_KEY,\n",
        "#     model=\"models/gemini-1.5-flash-latest\"\n",
        "# )\n",
        "\n",
        "# agent = ReActAgent.from_tools([player_tool, team_tool, get_matches_list_tool, get_series_squad_tool, get_match_scorecard_tool, get_t20_world_cup_stat_insights_tool], llm=llm, verbose=True, max_iterations=50)\n",
        "\n",
        "# agent.chat_repl()"
      ],
      "metadata": {
        "id": "-7x00Vi1JCnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our chatbot is now capabale of,\n",
        "\n",
        "1. Answering questions about player bios, stats, matches info etc from the world cup.\n",
        "2. It is capable of multi-turn conversation.\n",
        "3. It can handle multi-hop queries (queries which may need info from different datasources)\n",
        "\n",
        "Well done. You guys are awesome "
      ],
      "metadata": {
        "id": "RIPoi3Ttzdqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeing we have created our own chatbot with using almost less than 50 lines of code, what can be our next steps to making this more awesome  ?\n",
        "\n",
        "1. Create a **FastAPI** app which can expose our chatbot outside of this notebook\n",
        "\n",
        "> We have already implemented a small FastAPI app which can be starting point to extend and implement your own.\n",
        "You can find it here > https://github.com/happyfoxinc/agentic-rag-workshop/tree/main/webapp\n",
        "\n",
        "2. Your your dataset instead of the T20 world cup dataset we used. Try to play around with your data and see the power of AI."
      ],
      "metadata": {
        "id": "SmF6_ibr1sxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "kSRx0pe8_1xH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excercise 1\n",
        "\n",
        "In this exercise we can try to extend the capabilities of our `get_t20_world_cup_stat_insights` tool.\n",
        "\n",
        "\n",
        "Suggestions:\n",
        "\n",
        "1. Add a capability to get the player with the lowest score\n",
        "\n",
        "2. Add a capability to get the player with the least number of wickets\n",
        "\n",
        "3. Add a capability to get the player who has played the most number of balls"
      ],
      "metadata": {
        "id": "EwAEDiPo_4nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Existing tool code\n",
        "\n",
        "*Some tips for doing the execise*\n",
        "\n",
        "1. If you encounter any issues when performing operations on the dataset, check if the dataset has any inappropriate values and try to clean them.\n",
        "\n",
        "2. If there are any variable or import not found error, try running the cells above where the imports/variables are declared, or feel free to copy and paste them here.\n",
        "\n",
        "3. For operations than can be performed with pandas on a csv dataset, try reading throught their documentation and understand what is needed for completing this task."
      ],
      "metadata": {
        "id": "3CCGBcjaC4yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def get_t20_world_cup_stat_insights(stats_type: str, insight_type: str):\n",
        "  \"\"\"\n",
        "  This will return the requested insights for the the given stats_type\n",
        "  and insight_type\n",
        "\n",
        "  Supported stats_type:\n",
        "  1. batting\n",
        "  2. bowling\n",
        "\n",
        "  Supported insight_types\n",
        "  1. For batting\n",
        "    a. highest_run_scorer\n",
        "    b. player_with_highest_strike_rate\n",
        "  2. For bowling\n",
        "    a. highest_wicket_taker\n",
        "    b. player_with_least_economy\n",
        "  \"\"\"\n",
        "  if stats_type == \"batting\":\n",
        "    if insight_type == \"highest_run_scorer\":\n",
        "      row_with_highest_score = batting_df.loc[batting_df[\"Runs\"].idxmax()]\n",
        "\n",
        "      return f\"{row_with_highest_score['Player']} has scored {row_with_highest_score['Runs']} runs, which is the highest.\"\n",
        "    elif insight_type == \"player_with_highest_strike_rate\":\n",
        "      row_with_highest_strike_rate = batting_df.loc[batting_df[\"SR\"].idxmax()]\n",
        "\n",
        "      return f\"{row_with_highest_strike_rate['Player']} has {row_with_highest_strike_rate['SR']} strike rate, which is the highest.\"\n",
        "    else:\n",
        "      return \"Invalid insight_type for batting\"\n",
        "  elif stats_type == \"bowling\":\n",
        "    if insight_type == \"highest_wicket_taker\":\n",
        "      row_with_highest_wickets = bowling_df.loc[bowling_df[\"Wkts\"].idxmax()]\n",
        "\n",
        "      return f\"{row_with_highest_wickets['Player']} has taken {row_with_highest_wickets['Wkts']} wickets, which is the highest.\"\n",
        "    elif insight_type == \"player_with_least_economy\":\n",
        "      row_with_least_economy = bowling_df.loc[bowling_df[\"Econ\"].idxmin()]\n",
        "\n",
        "      return f\"{row_with_least_economy['Player']} has {row_with_least_economy['Econ']} economy, which is the least.\"\n",
        "    else:\n",
        "      return \"Invalid insight_type for bowling\"\n",
        "  else:\n",
        "    return \"Invalid stats_type\"\n",
        "\n",
        "\n",
        "get_t20_world_cup_stat_insights_tool = FunctionTool.from_defaults(fn=get_t20_world_cup_stat_insights)"
      ],
      "metadata": {
        "id": "52UhVsv5DCrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excercise 2\n",
        "\n",
        "In this exercise you are going to bring your own dataset into the table.\n",
        "\n",
        "Try to,\n",
        "\n",
        "1. Bring your own pdf dataset\n",
        "\n",
        "2. Process it, ingest and create an index for it.\n",
        "\n",
        "3. Create a query engine for the index and add it as an extra tool to our chatbot.\n",
        "\n",
        "Kindly, refer different parts of this notebook and [Llama Index Docs](https://docs.llamaindex.ai/en/stable/) to complete this exercie\n",
        "\n",
        "All the very best for doing these exercises. I hope you guys will have fun doing these."
      ],
      "metadata": {
        "id": "1DZd6ELAF3pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thanks everyone, you have been a great audience. We hope you guys had fun and learnt something valuable. "
      ],
      "metadata": {
        "id": "5QeD7Y7gLYCl"
      }
    }
  ]
}